---
title: "Biostat 203B Homework 4"
author: "Tomoki Okuno"
subtitle: Due Mar 18 @ 11:59PM
output:
  # ioslides_presentation: default
  html_document:
    toc: true
    toc_depth: 4
---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

Display machine information:
```{r}
sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(miceRanger))
```

## Q1. Missing data

Through the Shiny app developed in HW3, we observe abundant missing values in the MIMIC-IV ICU cohort we created. In this question, we use multiple imputation to obtain a data set without missing values.

0. Read following tutorials on the R package miceRanger for imputation: <https://github.com/farrellday/miceRanger>, <https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html>.

    A more thorough book treatment of the practical imputation strategies is the book [*_Flexible Imputation of Missing Data_*](https://stefvanbuuren.name/fimd/) by Stef van Buuren. 
    
 **Solution:**
Read the tutorial on miceRanger and some parts of the book by Stef van Buuren.

1. Explain the jargon MCAR, MAR, and MNAR.

**Solution:**  
These terminologies represent the missing data mechanism.

- MCAR stands for missing completely at random: Missingness is independent of any data. The type of data is rarely found in real data.
- MAR stands for missing at random. Missingness does not depend on the missing data but on the observed data. For instance, the older a person is, the more likely the data on income are to be missing. The probability of missing data is randomly generated conditional on the observed data.  
- MNAR stands for missing not at Random: Missingness depends on the missing data itself. For example, the lower your income is, the less likely you may be to report your income.

2. Explain in a couple of sentences how the Multiple Imputation by Chained Equations (MICE) work.

**Solution:**  
While the single imputation supplements each missing data with an imputed value as the true value, the multiple imputation iterates this procedure until convergence to integrate the given estimates to take into account the uncertainty due to missing values. MICE, one of the most typical methods, starts with a random draw from the observed data, and imputes missing values several times (e.g., 5 or 10 times) by regressing each missing variable on the remaining variables under the assumption of the missing data are MAR.

3. Perform a data quality check of the ICU stays data. Discard variables with substantial missingness, say >5000 `NA`s. Replace apparent data entry errors by `NA`s.

**Solution:**  
I deleted variables with more than 5000 `NA`s, which were `deathtime`, `edregtime`, `edouttime`, and `dod`. For the remaining numerical variables, values outside the range of [Q1 - 1.5 IQR, Q3 + 1.5 IQR] were replaced with `NA`s as outliers.
```{r}
# import ICU cohort data created in HW3
icu_cohort <- read_rds("../hw3/mimiciv_shiny/icu_cohort.rds")

# delete patients aged 18 since I followed the instruction to keep them in HW3
icu_cohort <- icu_cohort %>%
  filter(age_hadm > 18)
# check variables with more than 5000 missing values
icu_cohort %>%
  select_if(colSums(is.na(icu_cohort)) > 5000) %>%
  colnames()
# keep only variables with less than 5000 missing values
icu_cohort_discard <- icu_cohort %>%
  select_if(colSums(is.na(icu_cohort)) <= 5000) %>%
  print(width = Inf)
# make a function to replace outliers to `NA`s using IQR rule
outlier_to_na <- function(x) {
  qnt <- quantile(x, probs = c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  x[x < (qnt[1] - H)] <- NA
  x[x > (qnt[2] + H)] <- NA
  x
}
# replace outliers to `NA`s for numerical variables
icu_cohort_replace <- icu_cohort_discard %>% 
  mutate_if(is.numeric, outlier_to_na)
summary(icu_cohort_replace)
```


4. Impute missing values by `miceRanger` (request $m=3$ data sets). This step is computational intensive. Make sure to save the imputation results as a file. Hint: Setting `max.depth=10` in the `miceRanger` function may cut some computing time.

**Solution:**  
I kept only variables of interest and used `miceRanger` to save three imputed data sets to `icu_cohort_imputed.rds`.
```{r}
# choose variables of interest
demo_var = c("gender", "age_hadm", "marital_status", "ethnicity",
             "thirty_day_mort")
lab_item <- c(50912, 
              50971, 
              50983, 
              50902, 
              50882, 
              51221, 
              51301, 
              50931, 
              50960, 
              50893)
lab_item = paste("lab", lab_item, sep = "")
vital_item <- c(220045, 
                220181, 
                220179, 
                223761, 
                220210)
vital_item = paste("chart", vital_item, sep = "")
# merge all variables
all_vars = c(demo_var, lab_item, vital_item)
# kept only variables we plan to use
icu_cohort_mice <- icu_cohort_replace %>%
  select(all_of(all_vars))
summary(icu_cohort_mice)
```
```{r}
if (file.exists("icu_cohort_imputed.rds")) {
  icu_cohort_imputed <- read_rds("icu_cohort_imputed.rds")
} else {
  parTime <- system.time(
    icu_cohort_imputed <- miceRanger(
      icu_cohort_mice,
      m = 3,
      max.depth = 10,
      returnModels = FALSE,
      verbose = TRUE
    )
  )
  icu_cohort_imputed %>%
    write_rds("icu_cohort_imputed.rds")
}
```

5. Make imputation diagnostic plots and explain what they mean.

**Solution:** 
I output some imputation diagnostic plots to check the validation of the imputed distribution.

Distribution of Imputed Values:  
These plots help us compare the imputed distribution (black lines) to the original distribution (red line) for each variable. It appears that most variables are reasonably well matched. A few variables, such as 50983 (sodium), 220045 (heart rate), and 220181 (mean non-invasive BP), do not match, which implies that these data may be MNAR, say dependent on missing value itself:
```{r}
plotDistributions(icu_cohort_imputed, vars = 'allNumeric')
```

Model OOB Error: We can take a look at out-of-bag R-squared for regression by iteration. Other than the last three vitals, these variables appear to converge as they are repeated.
```{r}
plotModelError(icu_cohort_imputed,vars = 'allNumeric')
```

Variable Importance: The table below shows which variables were used to impute each variable. Overall, each lab measurement was important to other lab measurements, and so were vitals. In addition, gender and age at admission were also important for 50912 (creatinine).
```{r}
plotVarImportance(icu_cohort_imputed)
# plotCorrelations(icu_cohort_imputed, vars = 'allNumeric')
# plotVarConvergence(icu_cohort_imputed, vars = 'allNumeric')
# plotImputationVariance(icu_cohort_imputed, ncol = 2, widths = c(5,3))
```

6. Choose one of the imputed data sets to be used in Q2. This is **not** a good idea to use just one imputed data set or to average multiple imputed data sets. Explain in a couple of sentences what the correct Multiple Imputation strategy is.

**Solution:**  
The correct Multiple Imputation strategy is to pool $m$ imputed data sets, that is, integrate $m$ parameter estimates given by `miceRanger`, accounting for uncertainty (differences in variation) among imputed values. The variance combines the within-imputation and between-imputation variance. The pooled estimates are unbiased and have valid statistical properties compared to using one imputed data set or averaging multiple imputed data sets.
```{r}
# complete imputed data sets
icu_cohort_complete <- completeData(icu_cohort_imputed)
# choose one of the imputed data sets
icu_cohort_choice <- icu_cohort_complete[[1]]
```

**Data preparation:**
```{r}
# convert non-numeric variables to factors and scale numeric variables
icu_cohort_factor <- icu_cohort_choice %>%
  mutate_if(is.character, as.factor) %>%
  mutate_if(is.logical, as.factor) %>%
  mutate_if(is.numeric, scale)
```

## Q2. Predicting 30-day mortality

Develop at least two analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression (`glm()` function in base R or keras), (2) logistic regression with lasso penalty (glmnet or keras package), (3) random forest (randomForest package), or (4) neural network (keras package).

1. Partition data into 80% training set and 20% test set. Stratify partitioning according the 30-day mortality status.

**Solution:**  
I installed caret package to split data into 80% training set and 20% test set. By the way, caTools package can also be used.
```{r}
library(caret)
set.seed(3456)
index <- createDataPartition(icu_cohort_factor$thirty_day_mort,
                             p = .8, list = FALSE)
training_set <- icu_cohort_factor[index, ]
test_set <- icu_cohort_factor[-index, ]

# library(caTools)
# # split = sample.split(icu_cohort_factor$thirty_day_mort, SplitRatio = 0.8)
# # training_set = subset(icu_cohort_factor, split == TRUE)
# # test_set = subset(icu_cohort_factor, split == FALSE)
```

2. Train the models using the train set.
3. Compare model prediction performance on the test set.

**Solution:**  
**(1)** Logistic regression, using `glm()`.
```{r}
# fit the model
logstc_model <- glm(thirty_day_mort ~ ., training_set, family = binomial)
summary(logstc_model)
```
```{r}
# predict 30-day mortality using the model
prob_logstc <- predict(logstc_model, test_set, type = "response")
# translate probabilities to predictions by threshold = 0.5
predict_logstc <- ifelse(prob_logstc > 0.5, TRUE, FALSE)
# make confusion matrix
table(observed = test_set$thirty_day_mort, predicted = predict_logstc)
# compute accuracy 
cat('Test accuracy', mean(predict_logstc == test_set$thirty_day_mort))

# confusionMatrix(as.factor(predict_logstc),
#                 as.factor(test_set$thirty_day_mort))
```
Accuracy by prediction on the test set was about 90%, which changed little when the threshold was further increased.

**(2)** Logistic regression with lasso penalty using glmnet package
```{r}
library(glmnet)
# convert x to matrix and choose y
x_train <- model.matrix(thirty_day_mort ~ ., training_set)
x_test <- model.matrix(test_set$thirty_day_mort ~ ., test_set)
y_train <- training_set$thirty_day_mort
y_test <- test_set$thirty_day_mort
```
```{r}
# find optimal value of lambda (alpha=1 => lasso)
cv.out <- cv.glmnet(x_train, y_train, alpha = 1, 
                    family = "binomial", 
                    type.measure = "auc")
cv.out
# plot(cv.out)
# min value of lambda
lambda_min <- cv.out$lambda.min
# best value of lambda
lambda_1se <- cv.out$lambda.1se
# fit the model
coef(cv.out, s = lambda_1se)
```
```{r}
# predict 30-day mortality using the model
prob_lasso <- predict(cv.out, newx = x_test, s = lambda_1se, type = "response")
# translate probabilities to predictions
predict_lasso <- ifelse(prob_lasso > 0.5, TRUE, FALSE)
# make confusion matrix for the fitted value versus 30-day mortality in test
table(observed = y_test, predicted = predict_lasso)
# compute accuracy 
cat('Test accuracy', mean(predict_lasso == test_set$thirty_day_mort))
```
Accuracy by prediction on the test set was also about 90%.

**(3)** Random forest (randomForest package)
```{r}
library(randomForest)
# fit the model
model_rf <- randomForest(thirty_day_mort ~ ., data = training_set, ntree = 300)
# predict 30-day mortality using the model
prob_rf <- as.data.frame(predict(model_rf, test_set, type = "prob"))
predict_rf <- predict(model_rf, test_set)
# make confusion matrix
table(observed = test_set$thirty_day_mort, predicted = predict_rf)
# compute accuracy 
cat('Test accuracy', mean(predict_rf == test_set$thirty_day_mort))
```
Again, it was around 90% accuracy, which also remained unchanged even when the number of trees was increased.

**(4)** Neural network (keras package)
```{r}
# install_keras(tensorflow=2.6) 
# install_keras() 
# virtualenv_create("r-reticulate")
library(keras)
library(tensorflow)
```
```{r}
# data preparation for neural network
x_train_array <- array_reshape(x_train, c(nrow(x_train), 28))
x_test_array <- array_reshape(x_test, c(nrow(x_test), 28))
y_train_num <- as.numeric(y_train) - 1
y_test_num <- as.numeric(y_test) - 1
# encode y as binary class matrix
y_train_num <- to_categorical(y_train_num, 2)
y_test_num <- to_categorical(y_test_num, 2)
```
```{r}
# define the model
model_nn <- keras_model_sequential()
model_nn %>% 
  layer_dense(units = 20, activation = 'relu', input_shape = c(28)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 10, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = 'softmax')
  # layer_dense(input_shape = c(28), units = 2, activation = 'softmax')
summary(model_nn)

# compile the model
model_nn %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
# train and validate
history <- model_nn %>%
  fit(
    x_train_array, 
    y_train_num, 
    epochs = 200, 
    batch_size = 10000, 
    validation_spl1t = 0.2,
  )

# predict 30-day mortality using the model
predict_neural <- evaluate(model_nn, x_test_array, y_test_num) %>%
  print(width = Inf)
prob_neural <- predict(model_nn, x_test_array)
```
The neural network approach also provided roughly 90% accuracy, although I tried some sequential models.

**Conclusion:**  
The ROC curves for these approaches are shown below. In this data set, the random forest and the neural network methods indicated better prediction accuracy than the first two ones. This is because the closer that the curve is to the top-left corner, the better the model is. However, the data set was too simple for these two methods to have real power. Also, executing the correct multiple imputation strategy or dividing data into three to use a validation data set may improve the accuracy.
```{r}
# create ROC curve
library(ROCR)
pred1 <- prediction(prob_logstc, y_test)
pred2 <- prediction(prob_lasso, y_test)
pred3 <- prediction(prob_rf[2], y_test)
pred4 <- prediction(prob_neural[, 2], y_test)
perf1 <- performance(pred1, 'tpr', 'fpr')
perf2 <- performance(pred2, 'tpr', 'fpr')
perf3 <- performance(pred3, 'tpr', 'fpr')
perf4 <- performance(pred4, 'tpr', 'fpr')
plot(perf1, col = 4)
plot(perf2, add = T, col = 2)
plot(perf3, add = T, col = 3)
plot(perf4, add = T, col = 5)
title("ROC curves for the four approaches")
abline(a = 0, b = 1, lty = 2)
text(0.18, 0.4, 'Logistic', col = 4)
text(0.4, 0.55, 'Logistic with Lasso panelty', col = 2)
text(0.35, 0.95, 'Random forest', col = 3)
text(0.2, 0.8, 'Neural network', col = 5)
```